{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code in this file is modified code from\n",
    "\n",
    "#The Official Graphcast Repo\n",
    "#https://github.com/google-deepmind/graphcast\n",
    "\n",
    "#Graphcast: How to Get Things Done by Abhinav Kumar\n",
    "#https://towardsdatascience.com/graphcast-how-to-get-things-done-f2fd5630c5fb\n",
    "\n",
    "import os\n",
    "import cdsapi\n",
    "import datetime\n",
    "import isodate\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pysolar.radiation import get_radiation_direct\n",
    "from pysolar.solar import get_altitude\n",
    "import pytz\n",
    "import xarray\n",
    "from scipy import __name__ as scipy_name\n",
    "\n",
    "client = cdsapi.Client() # Making a connection to CDS, to fetch data.\n",
    "\n",
    "\n",
    "# The fields to be fetched from the single-level source.\n",
    "singlelevelfields = [\n",
    "                        '10m_u_component_of_wind',\n",
    "                        '10m_v_component_of_wind',\n",
    "                        '2m_temperature',\n",
    "                        'geopotential',\n",
    "                        'land_sea_mask',\n",
    "                        'mean_sea_level_pressure',\n",
    "                        'toa_incident_solar_radiation',\n",
    "                        'total_precipitation'\n",
    "                    ]\n",
    "\n",
    "# The fields to be fetched from the pressure-level source.\n",
    "pressurelevelfields = [\n",
    "                        'u_component_of_wind',\n",
    "                        'v_component_of_wind',\n",
    "                        'geopotential',\n",
    "                        'specific_humidity',\n",
    "                        'temperature',\n",
    "                        'vertical_velocity'\n",
    "                    ]\n",
    "\n",
    "# The 13 pressure levels.\n",
    "pressure_levels = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
    "\n",
    "# Initializing other required constants.\n",
    "pi = math.pi\n",
    "gap = 6 # There is a gap of 6 hours between each graphcast prediction.\n",
    "first_prediction = datetime.datetime(2024, 1, 1, 18, 0) # Timestamp of the first prediction.\n",
    "watts_to_joules = 3600\n",
    "predictions_steps = 12 # Predicting for 4 timestamps.\n",
    "lat_range = range(-180, 181, 1) # Latitude range.\n",
    "lon_range = range(0, 360, 1) # Longitude range.\n",
    "\n",
    "smallModel = False\n",
    "\n",
    "if smallModel == False:\n",
    "    spatial_resolution = '0.25/0.25'\n",
    "else:\n",
    "    spatial_resolution = '1.0/1.0'\n",
    "\n",
    "\n",
    "# A utility function used for ease of coding.\n",
    "# Converting the variable to a datetime object.\n",
    "def toDatetime(dt) -> datetime.datetime:\n",
    "    if isinstance(dt, datetime.date) and isinstance(dt, datetime.datetime):\n",
    "        return dt\n",
    "    \n",
    "    elif isinstance(dt, datetime.date) and not isinstance(dt, datetime.datetime):\n",
    "        return datetime.datetime.combine(dt, datetime.datetime.min.time())\n",
    "    \n",
    "    elif isinstance(dt, str):\n",
    "        if 'T' in dt:\n",
    "            return isodate.parse_datetime(dt)\n",
    "        else:\n",
    "            return datetime.datetime.combine(isodate.parse_date(dt), datetime.datetime.min.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Functions taken from the official Graphcast REPO  - Function from \"data_utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m SEC_PER_DAY \u001b[38;5;241m=\u001b[39m _SEC_PER_HOUR \u001b[38;5;241m*\u001b[39m _HOUR_PER_DAY\n\u001b[1;32m      6\u001b[0m _AVG_DAY_PER_YEAR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m365.24219\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_year_progress\u001b[39m(seconds_since_epoch: \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Computes year progress for times in seconds.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Year progress normalized to be in the [0, 1) interval for each time point.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;66;03m# Start with the pure integer division, and then float at the very end.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m   \u001b[38;5;66;03m# We will try to keep as much precision as possible.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Graphcast Function from \"data_utils\"\n",
    "_SEC_PER_HOUR = 3600\n",
    "_HOUR_PER_DAY = 24\n",
    "SEC_PER_DAY = _SEC_PER_HOUR * _HOUR_PER_DAY\n",
    "_AVG_DAY_PER_YEAR = 365.24219\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_year_progress(seconds_since_epoch: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"Computes year progress for times in seconds.\n",
    "\n",
    "  Args:\n",
    "    seconds_since_epoch: Times in seconds since the \"epoch\" (the point at which\n",
    "      UNIX time starts).\n",
    "\n",
    "  Returns:\n",
    "    Year progress normalized to be in the [0, 1) interval for each time point.\n",
    "  \"\"\"\n",
    "\n",
    "  # Start with the pure integer division, and then float at the very end.\n",
    "  # We will try to keep as much precision as possible.\n",
    "  years_since_epoch = (\n",
    "      seconds_since_epoch / SEC_PER_DAY / np.float64(_AVG_DAY_PER_YEAR)\n",
    "  )\n",
    "  # Note depending on how these ops are down, we may end up with a \"weak_type\"\n",
    "  # which can cause issues in subtle ways, and hard to track here.\n",
    "  # In any case, casting to float32 should get rid of the weak type.\n",
    "  # [0, 1.) Interval.\n",
    "  return np.mod(years_since_epoch, 1.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def get_day_progress(\n",
    "    seconds_since_epoch: np.ndarray,\n",
    "    longitude: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "  \"\"\"Computes day progress for times in seconds at each longitude.\n",
    "\n",
    "  Args:\n",
    "    seconds_since_epoch: 1D array of times in seconds since the 'epoch' (the\n",
    "      point at which UNIX time starts).\n",
    "    longitude: 1D array of longitudes at which day progress is computed.\n",
    "\n",
    "  Returns:\n",
    "    2D array of day progress values normalized to be in the [0, 1) inverval\n",
    "      for each time point at each longitude.\n",
    "  \"\"\"\n",
    "\n",
    "  # [0.0, 1.0) Interval.\n",
    "  day_progress_greenwich = (\n",
    "      np.mod(seconds_since_epoch, SEC_PER_DAY) / SEC_PER_DAY\n",
    "  )\n",
    "\n",
    "  # Offset the day progress to the longitude of each point on Earth.\n",
    "  longitude_offsets = np.deg2rad(longitude) / (2 * np.pi)\n",
    "  day_progress = np.mod(\n",
    "      day_progress_greenwich[..., np.newaxis] + longitude_offsets, 1.0\n",
    "  )\n",
    "  return day_progress.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Download data from the CDSAPI\n",
    " ~WARNING ENSURE 'inputs_data.pkl'  is deleted if you have made changes to the data you wish to have downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file...\n",
      "Data processing complete and saved.\n"
     ]
    }
   ],
   "source": [
    "# File paths for saved data     \n",
    "data_file = 'inputs_data.pkl' \n",
    "\n",
    "# Function to check for file existence and load or retrieve data\n",
    "def load_or_retrieve_data():\n",
    "    if os.path.exists(data_file):\n",
    "        print(\"Loading data from file...\")\n",
    "        values = pd.read_pickle(data_file)\n",
    "    else:\n",
    "        print(\"File not found, retrieving data...\")\n",
    "        single, pressure = getSingleAndPressureValues()\n",
    "        inputs = pd.merge(pressure, single, left_index=True, right_index=True, how='inner')\n",
    "        inputs = integrateProgress(inputs)\n",
    "        inputs = formatData(inputs)\n",
    "        values = {'inputs': inputs}  # Store the DataFrame in a dictionary\n",
    "        pd.to_pickle(values, data_file)  # Save to file for future runs\n",
    "        print(\"Data saved to file.\")\n",
    "    return values\n",
    "\n",
    "# Getting the single and pressure level values.\n",
    "def getSingleAndPressureValues():\n",
    "    client.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': singlelevelfields,\n",
    "            'grid': spatial_resolution,\n",
    "            'year': [2024],\n",
    "            'month': [1],\n",
    "            'day': [1],\n",
    "            'time': ['00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00', '12:00'],\n",
    "            'format': 'netcdf'\n",
    "        },\n",
    "        'single-level.nc'\n",
    "    )\n",
    "    singlelevel = xarray.open_dataset('single-level.nc', engine=scipy_name).to_dataframe()\n",
    "    singlelevel = singlelevel.rename(columns={col: singlelevelfields[ind] for ind, col in enumerate(singlelevel.columns.values.tolist())})\n",
    "    singlelevel = singlelevel.rename(columns={'geopotential': 'geopotential_at_surface'})\n",
    "\n",
    "    # Calculating the sum of the last 6 hours of rainfall.\n",
    "    singlelevel = singlelevel.sort_index()\n",
    "    singlelevel['total_precipitation_6hr'] = singlelevel.groupby(level=[0, 1])['total_precipitation'].rolling(window=6, min_periods=1).sum().reset_index(level=[0, 1], drop=True)\n",
    "    singlelevel.pop('total_precipitation')\n",
    "\n",
    "    client.retrieve(\n",
    "        'reanalysis-era5-pressure-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': pressurelevelfields,\n",
    "            'grid': spatial_resolution,\n",
    "            'year': [2024],\n",
    "            'month': [1],\n",
    "            'day': [1],\n",
    "            'time': ['06:00', '12:00'],\n",
    "            'pressure_level': pressure_levels,\n",
    "            'format': 'netcdf'\n",
    "        },\n",
    "        'pressure-level.nc'\n",
    "    )\n",
    "    pressurelevel = xarray.open_dataset('pressure-level.nc', engine=scipy_name).to_dataframe()\n",
    "    pressurelevel = pressurelevel.rename(columns={col: pressurelevelfields[ind] for ind, col in enumerate(pressurelevel.columns.values.tolist())})\n",
    "\n",
    "    return singlelevel, pressurelevel\n",
    "\n",
    "# Adding sin and cos of the year progress.\n",
    "def addYearProgress(secs, data):\n",
    "    progress = get_year_progress(secs)\n",
    "    data['year_progress_sin'] = math.sin(2 * math.pi * progress)\n",
    "    data['year_progress_cos'] = math.cos(2 * math.pi * progress)\n",
    "    return data\n",
    "\n",
    "# Adding sin and cos of the day progress.\n",
    "def addDayProgress(secs, lon: str, data: pd.DataFrame):\n",
    "    lons = data.index.get_level_values(lon).unique()\n",
    "    progress: np.ndarray = get_day_progress(secs, np.array(lons))\n",
    "    prxlon = {lon: prog for lon, prog in list(zip(list(lons), progress.tolist()))}\n",
    "    data['day_progress_sin'] = data.index.get_level_values(lon).map(lambda x: math.sin(2 * math.pi * prxlon[x]))\n",
    "    data['day_progress_cos'] = data.index.get_level_values(lon).map(lambda x: math.cos(2 * math.pi * prxlon[x]))\n",
    "    return data\n",
    "\n",
    "# Adding day and year progress.\n",
    "def integrateProgress(data: pd.DataFrame):\n",
    "    for dt in data.index.get_level_values('time').unique():\n",
    "        seconds_since_epoch = toDatetime(dt).timestamp()\n",
    "        data = addYearProgress(seconds_since_epoch, data)\n",
    "        data = addDayProgress(seconds_since_epoch, 'longitude' if 'longitude' in data.index.names else 'lon', data)\n",
    "    return data\n",
    "\n",
    "# Adding batch field and renaming some others.\n",
    "def formatData(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.rename_axis(index={'latitude': 'lat', 'longitude': 'lon'})\n",
    "    if 'batch' not in data.index.names:\n",
    "        data['batch'] = 0\n",
    "        data = data.set_index('batch', append=True)\n",
    "    return data\n",
    "\n",
    "# Main function to run the process\n",
    "if __name__ == '__main__':\n",
    "    values = load_or_retrieve_data()\n",
    "    print(\"Data processing complete and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Includes the packages imported and constants assigned.\n",
    "# The functions created for the inputs also go here.\n",
    "\n",
    "predictionFields = [\n",
    "                        'u_component_of_wind',\n",
    "                        'v_component_of_wind',\n",
    "                        'geopotential',\n",
    "                        'specific_humidity',\n",
    "                        'temperature',\n",
    "                        'vertical_velocity',\n",
    "                        '10m_u_component_of_wind',\n",
    "                        '10m_v_component_of_wind',\n",
    "                        '2m_temperature',\n",
    "                        'mean_sea_level_pressure',\n",
    "                        'total_precipitation_6hr'\n",
    "                    ]\n",
    "\n",
    "# Creating an array full of nan values.\n",
    "def nans(*args) -> list:\n",
    "    return np.full((args), np.nan)\n",
    "\n",
    "# Adding or subtracting time.\n",
    "def deltaTime(dt, **delta) -> datetime.datetime:\n",
    "    return dt + datetime.timedelta(**delta)\n",
    "\n",
    "def getTargets(dt, data:pd.DataFrame):\n",
    "    \n",
    "    print(\"1\")\n",
    "    # Creating an array consisting of unique values of each index.\n",
    "    lat, lon, levels, batch = sorted(data.index.get_level_values('lat').unique().tolist()), sorted(data.index.get_level_values('lon').unique().tolist()), sorted(data.index.get_level_values('level').unique().tolist()), data.index.get_level_values('batch').unique().tolist()\n",
    "    time = [deltaTime(dt, hours = days * gap) for days in range(4)] #harcoded as 4\n",
    "    #time = [deltaTime(dt, hours = days * gap) for days in range(predictions_steps)] #Fixed\n",
    "\n",
    "    # Creating an empty dataset using latitude, longitude, the pressure levels and each prediction timestamp.\n",
    "    target = xarray.Dataset({field: (['lat', 'lon', 'level', 'time'], nans(len(lat), len(lon), len(levels), len(time))) for field in predictionFields}, coords = {'lat': lat, 'lon': lon, 'level': levels, 'time': time, 'batch': batch})\n",
    "\n",
    "    return target.to_dataframe()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # The code for creating inputs will be here.\n",
    "\n",
    "    values['targets'] = getTargets(first_prediction, values['inputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes the packages imported and constants assigned.\n",
    "# The functions created for the inputs and targets also go here.\n",
    "\n",
    "# Adding a timezone to datetime.datetime variables.\n",
    "def addTimezone(dt, tz = pytz.UTC) -> datetime.datetime:\n",
    "    dt = toDatetime(dt)\n",
    "    if dt.tzinfo == None:\n",
    "        return pytz.UTC.localize(dt).astimezone(tz)\n",
    "    else:\n",
    "        return dt.astimezone(tz)\n",
    "\n",
    "# Getting the solar radiation value wrt longitude, latitude and timestamp.\n",
    "def getSolarRadiation(longitude, latitude, dt):\n",
    "        \n",
    "    altitude_degrees = get_altitude(latitude, longitude, addTimezone(dt))\n",
    "    solar_radiation = get_radiation_direct(dt, altitude_degrees) if altitude_degrees > 0 else 0\n",
    "\n",
    "    return solar_radiation * watts_to_joules\n",
    "\n",
    "# Calculating the solar radiation values for timestamps to be predicted.\n",
    "def integrateSolarRadiation(data:pd.DataFrame):\n",
    "    \n",
    "    dates = list(data.index.get_level_values('time').unique())\n",
    "    coords = [[lat, lon] for lat in lat_range for lon in lon_range]\n",
    "    values = []\n",
    "    \n",
    "    # For each data, getting the solar radiation value at a particular coordinate.\n",
    "    for dt in dates:\n",
    "        values.extend(list(map(lambda coord:{'time': dt, 'lon': coord[1], 'lat': coord[0], 'toa_incident_solar_radiation': getSolarRadiation(coord[1], coord[0], dt)}, coords)))\n",
    "  \n",
    "    # Setting indices.\n",
    "    values = pd.DataFrame(values).set_index(keys = ['lat', 'lon', 'time'])\n",
    "      \n",
    "    # The forcings dataset will now contain the solar radiation values.\n",
    "    return pd.merge(data, values, left_index = True, right_index = True, how = 'inner')\n",
    "\n",
    "def getForcings(data:pd.DataFrame):\n",
    "  \n",
    "    # Since forcings data does not contain batch as an index, it is dropped.\n",
    "    # So are all the columns, since forcings data only has 5, which will be created.\n",
    "    forcingdf = data.reset_index(level = 'level', drop = True).drop(labels = predictionFields, axis = 1)\n",
    "    \n",
    "    # Keeping only the unique indices.\n",
    "    forcingdf = pd.DataFrame(index = forcingdf.index.drop_duplicates(keep = 'first'))\n",
    "\n",
    "    # Adding the sin and cos of day and year progress.\n",
    "    # Functions are included in the creation of inputs data section.\n",
    "    forcingdf = integrateProgress(forcingdf)\n",
    "\n",
    "    # Integrating the solar radiation values.\n",
    "    forcingdf = integrateSolarRadiation(forcingdf)\n",
    "\n",
    "    return forcingdf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # The code for creating inputs and targets will be here.\n",
    "\n",
    "    values['forcings'] = getForcings(values['targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes the packages imported and constants assigned.\n",
    "# The functions created for the inputs, targets and forcings also go here.\n",
    "\n",
    "# A dictionary created, containing each coordinate a data variable requires.\n",
    "class AssignCoordinates:\n",
    "    \n",
    "    coordinates = {\n",
    "                    '2m_temperature': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'mean_sea_level_pressure': ['batch', 'lon', 'lat', 'time'],\n",
    "                    '10m_v_component_of_wind': ['batch', 'lon', 'lat', 'time'],\n",
    "                    '10m_u_component_of_wind': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'total_precipitation_6hr': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'temperature': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'geopotential': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'u_component_of_wind': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'v_component_of_wind': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'vertical_velocity': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'specific_humidity': ['batch', 'lon', 'lat', 'level', 'time'],\n",
    "                    'toa_incident_solar_radiation': ['batch', 'lon', 'lat', 'time'],\n",
    "                    'year_progress_cos': ['batch', 'time'],\n",
    "                    'year_progress_sin': ['batch', 'time'],\n",
    "                    'day_progress_cos': ['batch', 'lon', 'time'],\n",
    "                    'day_progress_sin': ['batch', 'lon', 'time'],\n",
    "                    'geopotential_at_surface': ['lon', 'lat'],\n",
    "                    'land_sea_mask': ['lon', 'lat'],\n",
    "                }\n",
    "\n",
    "def modifyCoordinates(data:xarray.Dataset):\n",
    "    \n",
    "    # Parsing through each data variable and removing unneeded indices.\n",
    "    for var in list(data.data_vars):\n",
    "        varArray:xarray.DataArray = data[var]\n",
    "        nonIndices = list(set(list(varArray.coords)).difference(set(AssignCoordinates.coordinates[var])))\n",
    "        data[var] = varArray.isel(**{coord: 0 for coord in nonIndices})\n",
    "    data = data.drop_vars('batch')\n",
    "\n",
    "    return data\n",
    "\n",
    "def makeXarray(data:pd.DataFrame) -> xarray.Dataset:\n",
    "    \n",
    "    # Converting to xarray.\n",
    "    data = data.to_xarray()\n",
    "    data = modifyCoordinates(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # The code for creating inputs, targets and forcings will be here.\n",
    "    values = {value:makeXarray(values[value]) for value in values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of data in 'inputs':\n",
      "<xarray.Dataset> Size: 15kB\n",
      "Dimensions:                       (lon: 5, lat: 5, level: 5, time: 2, batch: 1)\n",
      "Coordinates:\n",
      "  * lon                           (lon) float32 20B 0.0 0.25 0.5 0.75 1.0\n",
      "  * lat                           (lat) float32 20B -90.0 -89.75 ... -89.0\n",
      "  * level                         (level) int32 20B 50 100 150 200 250\n",
      "  * time                          (time) datetime64[ns] 16B 2024-01-01T06:00:...\n",
      "Dimensions without coordinates: batch\n",
      "Data variables: (12/18)\n",
      "    u_component_of_wind           (lon, lat, level, time, batch) float64 2kB ...\n",
      "    v_component_of_wind           (lon, lat, level, time, batch) float64 2kB ...\n",
      "    geopotential                  (lon, lat, level, time, batch) float64 2kB ...\n",
      "    specific_humidity             (lon, lat, level, time, batch) float64 2kB ...\n",
      "    temperature                   (lon, lat, level, time, batch) float64 2kB ...\n",
      "    vertical_velocity             (lon, lat, level, time, batch) float64 2kB ...\n",
      "    ...                            ...\n",
      "    toa_incident_solar_radiation  (lon, lat, time, batch) float64 400B 1.997e...\n",
      "    total_precipitation_6hr       (lon, lat, time, batch) float64 400B 0.0003...\n",
      "    year_progress_sin             (time, batch) float64 16B 0.007255 0.007255\n",
      "    year_progress_cos             (time, batch) float64 16B 1.0 1.0\n",
      "    day_progress_sin              (lon, time, batch) float32 40B 1.225e-16 .....\n",
      "    day_progress_cos              (lon, time, batch) float32 40B -1.0 ... -0....\n",
      "\n",
      "Sample of data in 'targets':\n",
      "<xarray.Dataset> Size: 28kB\n",
      "Dimensions:                  (lat: 5, lon: 5, level: 5, time: 4, batch: 1)\n",
      "Coordinates:\n",
      "  * lat                      (lat) float64 40B -90.0 -89.75 -89.5 -89.25 -89.0\n",
      "  * lon                      (lon) float64 40B 0.0 0.25 0.5 0.75 1.0\n",
      "  * level                    (level) int64 40B 50 100 150 200 250\n",
      "  * time                     (time) datetime64[ns] 32B 2024-01-01T18:00:00 .....\n",
      "Dimensions without coordinates: batch\n",
      "Data variables:\n",
      "    u_component_of_wind      (lat, lon, level, time, batch) float64 4kB nan ....\n",
      "    v_component_of_wind      (lat, lon, level, time, batch) float64 4kB nan ....\n",
      "    geopotential             (lat, lon, level, time, batch) float64 4kB nan ....\n",
      "    specific_humidity        (lat, lon, level, time, batch) float64 4kB nan ....\n",
      "    temperature              (lat, lon, level, time, batch) float64 4kB nan ....\n",
      "    vertical_velocity        (lat, lon, level, time, batch) float64 4kB nan ....\n",
      "    10m_u_component_of_wind  (lat, lon, time, batch) float64 800B nan ... nan\n",
      "    10m_v_component_of_wind  (lat, lon, time, batch) float64 800B nan ... nan\n",
      "    2m_temperature           (lat, lon, time, batch) float64 800B nan ... nan\n",
      "    mean_sea_level_pressure  (lat, lon, time, batch) float64 800B nan ... nan\n",
      "    total_precipitation_6hr  (lat, lon, time, batch) float64 800B nan ... nan\n",
      "\n",
      "Sample of data in 'forcings':\n",
      "<xarray.Dataset> Size: 1kB\n",
      "Dimensions:                       (time: 4, batch: 1, lon: 5, lat: 5)\n",
      "Coordinates:\n",
      "  * lat                           (lat) float64 40B -90.0 -89.0 ... -87.0 -86.0\n",
      "  * lon                           (lon) float64 40B 0.0 1.0 2.0 3.0 4.0\n",
      "  * time                          (time) datetime64[ns] 32B 2024-01-01T18:00:...\n",
      "Dimensions without coordinates: batch\n",
      "Data variables:\n",
      "    year_progress_sin             (time, batch) float64 32B 0.02446 ... 0.02446\n",
      "    year_progress_cos             (time, batch) float64 32B 0.9997 ... 0.9997\n",
      "    day_progress_sin              (lon, time, batch) float64 160B 1.225e-16 ....\n",
      "    day_progress_cos              (lon, time, batch) float64 160B -1.0 ... -0...\n",
      "    toa_incident_solar_radiation  (lat, lon, time, batch) float64 800B 3.114e...\n"
     ]
    }
   ],
   "source": [
    "for key, value in values.items():\n",
    "    print(f\"\\nSample of data in '{key}':\")\n",
    "    print(value.head())  # If it's a DataFrame or something with a similar method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF file saved successfully for 'inputs' as inputs_output_data.nc.\n",
      "NetCDF file saved successfully for 'targets' as targets_output_data.nc.\n",
      "NetCDF file saved successfully for 'forcings' as forcings_output_data.nc.\n"
     ]
    }
   ],
   "source": [
    "# Assuming `values` is a dictionary of xarray.Datasets\n",
    "for key, dataset in values.items():\n",
    "    if isinstance(dataset, xarray.Dataset):\n",
    "        # Create a unique filename for each dataset\n",
    "        output_file = f'{key}_output_data.nc'\n",
    "        \n",
    "        # Save the xarray Dataset to a NetCDF file\n",
    "        dataset.to_netcdf(output_file)\n",
    "        \n",
    "        print(f\"NetCDF file saved successfully for '{key}' as {output_file}.\")\n",
    "    else:\n",
    "        print(f\"Error: '{key}' is not an xarray.Dataset. It is {type(dataset)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs dataset:\n",
      "<xarray.Dataset> Size: 1GB\n",
      "Dimensions:                       (lon: 1440, lat: 721, level: 13, time: 2,\n",
      "                                   batch: 1)\n",
      "Coordinates:\n",
      "  * lon                           (lon) float32 6kB 0.0 0.25 0.5 ... 359.5 359.8\n",
      "  * lat                           (lat) float32 3kB -90.0 -89.75 ... 89.75 90.0\n",
      "  * level                         (level) int32 52B 50 100 150 ... 850 925 1000\n",
      "  * time                          (time) datetime64[ns] 16B 2024-01-01T06:00:...\n",
      "Dimensions without coordinates: batch\n",
      "Data variables: (12/18)\n",
      "    u_component_of_wind           (lon, lat, level, time, batch) float64 216MB ...\n",
      "    v_component_of_wind           (lon, lat, level, time, batch) float64 216MB ...\n",
      "    geopotential                  (lon, lat, level, time, batch) float64 216MB ...\n",
      "    specific_humidity             (lon, lat, level, time, batch) float64 216MB ...\n",
      "    temperature                   (lon, lat, level, time, batch) float64 216MB ...\n",
      "    vertical_velocity             (lon, lat, level, time, batch) float64 216MB ...\n",
      "    ...                            ...\n",
      "    toa_incident_solar_radiation  (lon, lat, time, batch) float64 17MB ...\n",
      "    total_precipitation_6hr       (lon, lat, time, batch) float64 17MB ...\n",
      "    year_progress_sin             (time, batch) float64 16B ...\n",
      "    year_progress_cos             (time, batch) float64 16B ...\n",
      "    day_progress_sin              (lon, time, batch) float32 12kB ...\n",
      "    day_progress_cos              (lon, time, batch) float32 12kB ...\n",
      "\n",
      "Targets dataset:\n",
      "<xarray.Dataset> Size: 3GB\n",
      "Dimensions:                  (lat: 721, lon: 1440, level: 13, time: 4, batch: 1)\n",
      "Coordinates:\n",
      "  * lat                      (lat) float64 6kB -90.0 -89.75 -89.5 ... 89.75 90.0\n",
      "  * lon                      (lon) float64 12kB 0.0 0.25 0.5 ... 359.5 359.8\n",
      "  * level                    (level) int64 104B 50 100 150 200 ... 850 925 1000\n",
      "  * time                     (time) datetime64[ns] 32B 2024-01-01T18:00:00 .....\n",
      "Dimensions without coordinates: batch\n",
      "Data variables:\n",
      "    u_component_of_wind      (lat, lon, level, time, batch) float64 432MB ...\n",
      "    v_component_of_wind      (lat, lon, level, time, batch) float64 432MB ...\n",
      "    geopotential             (lat, lon, level, time, batch) float64 432MB ...\n",
      "    specific_humidity        (lat, lon, level, time, batch) float64 432MB ...\n",
      "    temperature              (lat, lon, level, time, batch) float64 432MB ...\n",
      "    vertical_velocity        (lat, lon, level, time, batch) float64 432MB ...\n",
      "    10m_u_component_of_wind  (lat, lon, time, batch) float64 33MB ...\n",
      "    10m_v_component_of_wind  (lat, lon, time, batch) float64 33MB ...\n",
      "    2m_temperature           (lat, lon, time, batch) float64 33MB ...\n",
      "    mean_sea_level_pressure  (lat, lon, time, batch) float64 33MB ...\n",
      "    total_precipitation_6hr  (lat, lon, time, batch) float64 33MB ...\n",
      "\n",
      "Forcings dataset:\n",
      "<xarray.Dataset> Size: 2MB\n",
      "Dimensions:                       (time: 4, batch: 1, lon: 360, lat: 181)\n",
      "Coordinates:\n",
      "  * lat                           (lat) float64 1kB -90.0 -89.0 ... 89.0 90.0\n",
      "  * lon                           (lon) float64 3kB 0.0 1.0 2.0 ... 358.0 359.0\n",
      "  * time                          (time) datetime64[ns] 32B 2024-01-01T18:00:...\n",
      "Dimensions without coordinates: batch\n",
      "Data variables:\n",
      "    year_progress_sin             (time, batch) float64 32B ...\n",
      "    year_progress_cos             (time, batch) float64 32B ...\n",
      "    day_progress_sin              (lon, time, batch) float64 12kB ...\n",
      "    day_progress_cos              (lon, time, batch) float64 12kB ...\n",
      "    toa_incident_solar_radiation  (lat, lon, time, batch) float64 2MB ...\n"
     ]
    }
   ],
   "source": [
    "#Combine the Files\n",
    "# Define file paths for the NetCDF files\n",
    "input_file = 'inputs_output_data.nc'\n",
    "target_file = 'targets_output_data.nc'\n",
    "forcing_file = 'forcings_output_data.nc'\n",
    "\n",
    "# Load the NetCDF files into xarray Datasets\n",
    "inputs = xarray.open_dataset(input_file)\n",
    "targets = xarray.open_dataset(target_file)\n",
    "forcings = xarray.open_dataset(forcing_file)\n",
    "\n",
    "# Print the structure of each dataset to verify they are loaded correctly\n",
    "print(\"Inputs dataset:\")\n",
    "print(inputs)\n",
    "\n",
    "print(\"\\nTargets dataset:\")\n",
    "print(targets)\n",
    "\n",
    "print(\"\\nForcings dataset:\")\n",
    "print(forcings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format and export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1045475/2189697564.py:20: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  datetime_1d = pd.date_range(start=\"2024-01-01\", periods=time_dim, freq='6H').values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined dataset after fixing 'datetime' dimensions (batch, time):\n",
      "<xarray.Dataset> Size: 4GB\n",
      "Dimensions:                       (lon: 1440, lat: 721, level: 13, time: 6,\n",
      "                                   batch: 1)\n",
      "Coordinates:\n",
      "  * lon                           (lon) float64 12kB 0.0 0.25 ... 359.5 359.8\n",
      "  * lat                           (lat) float64 6kB -90.0 -89.75 ... 89.75 90.0\n",
      "  * level                         (level) int32 52B 50 100 150 ... 850 925 1000\n",
      "  * time                          (time) timedelta64[ns] 48B 00:00:00 ... 1 d...\n",
      "    datetime                      (batch, time) datetime64[ns] 48B 2024-01-01...\n",
      "Dimensions without coordinates: batch\n",
      "Data variables: (12/18)\n",
      "    u_component_of_wind           (lon, lat, level, time, batch) float64 648MB ...\n",
      "    v_component_of_wind           (lon, lat, level, time, batch) float64 648MB ...\n",
      "    geopotential                  (lon, lat, level, time, batch) float64 648MB ...\n",
      "    specific_humidity             (lon, lat, level, time, batch) float64 648MB ...\n",
      "    temperature                   (lon, lat, level, time, batch) float64 648MB ...\n",
      "    vertical_velocity             (lon, lat, level, time, batch) float64 648MB ...\n",
      "    ...                            ...\n",
      "    toa_incident_solar_radiation  (lon, lat, time, batch) float64 50MB 1.997e...\n",
      "    total_precipitation_6hr       (lon, lat, time, batch) float64 50MB 0.0003...\n",
      "    year_progress_sin             (time, batch) float64 48B 0.007255 ... 0.02446\n",
      "    year_progress_cos             (time, batch) float64 48B 1.0 1.0 ... 0.9997\n",
      "    day_progress_sin              (lon, time, batch) float64 69kB 1.225e-16 ....\n",
      "    day_progress_cos              (lon, time, batch) float64 69kB -1.0 ... nan\n",
      "\n",
      "Combined NetCDF file saved as source-era5_date-2024-01-01_res-0.25_levels-13_steps-12.nc.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Merge the datasets\n",
    "combined_dataset = xarray.merge([inputs, targets, forcings])\n",
    "\n",
    "batch_dim = combined_dataset.sizes['batch']  # Number of batches\n",
    "time_dim = combined_dataset.sizes['time']    # Number of time steps\n",
    "\n",
    "# Step 2: Fix 'time' coordinate (convert datetime64[ns] to timedelta64[ns])\n",
    "# Get the original 'time' values (which are in datetime64[ns])\n",
    "datetime_values = combined_dataset['time'].values\n",
    "\n",
    "# Calculate the difference between each time value and the first time value (to get timedelta)\n",
    "timedelta_values = datetime_values - datetime_values[0]\n",
    "\n",
    "# Update the 'time' coordinate in the dataset with the timedelta values (timedelta64[ns])\n",
    "combined_dataset['time'] = xarray.DataArray(timedelta_values, dims=\"time\")\n",
    "\n",
    "# Step 3: Fix 'datetime' by aligning it only to 'batch' and 'time'\n",
    "# Create a 1D array of datetime values with a frequency of 6 hours (assuming 6 time steps)\n",
    "datetime_1d = pd.date_range(start=\"2024-01-01\", periods=time_dim, freq='6H').values\n",
    "\n",
    "# Expand to 2D with the correct batch and time dimensions\n",
    "datetime_2d = xarray.DataArray(datetime_1d, dims=[\"time\"]).expand_dims({\"batch\": batch_dim}, axis=0)\n",
    "\n",
    "# Extract the underlying NumPy data from the DataArray\n",
    "datetime_2d_data = datetime_2d.data\n",
    "\n",
    "# Remove the unnecessary dimensions from 'datetime' and assign it properly\n",
    "combined_dataset = combined_dataset.assign_coords(datetime=([\"batch\", \"time\"], datetime_2d_data))\n",
    "\n",
    "# Print the structure after fixing 'datetime'\n",
    "print(\"\\nCombined dataset after fixing 'datetime' dimensions (batch, time):\")\n",
    "print(combined_dataset)\n",
    "\n",
    "# Step 4: Save the final combined dataset to a NetCDF file\n",
    "combined_output_file = 'source-era5_date-2024-01-01_res-0.25_levels-13_steps-12.nc' # this needs to be dynamic for \n",
    "combined_dataset.to_netcdf(combined_output_file)\n",
    "print(f\"\\nCombined NetCDF file saved as {combined_output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphEnv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
